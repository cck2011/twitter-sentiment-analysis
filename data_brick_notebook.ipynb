{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753d0b22-c2dd-4deb-b888-88a5846f004d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('WCD Big Data Course') \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "print('Session created')\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74476230-85fe-4467-9512-6f4dfa1294f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mount_s3_bucket(access_key, secret_key, bucket_name, mount_folder):\n",
    "  ACCESS_KEY_ID = access_key\n",
    "  SECRET_ACCESS_KEY = secret_key\n",
    "  ENCODED_SECRET_KEY = SECRET_ACCESS_KEY.replace(\"/\", \"%2F\")\n",
    "\n",
    "  print (\"Mounting\", bucket_name)\n",
    "\n",
    "  try:\n",
    "    # Unmount the data in case it was already mounted.\n",
    "    dbutils.fs.unmount(\"/mnt/%s\" % mount_folder)\n",
    "    \n",
    "  except:\n",
    "    # If it fails to unmount it most likely wasn't mounted in the first place\n",
    "    print (\"Directory not unmounted: \", mount_folder)\n",
    "    \n",
    "  finally:\n",
    "    # Lastly, mount our bucket.\n",
    "    dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY_ID, ENCODED_SECRET_KEY, bucket_name), \"/mnt/%s\" % mount_folder)\n",
    "    #dbutils.fs.mount(\"s3a://\"+ ACCESS_KEY_ID + \":\" + ENCODED_SECRET_KEY + \"@\" + bucket_name, mount_folder)\n",
    "    print (\"The bucket\", bucket_name, \"was mounted to\", mount_folder, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56652e70-bc24-4955-af68-2b150c520d2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS programmatic access credentials\n",
    "ACCESS_KEY = \"\"\n",
    "SECRET_ACCESS_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96e74a8-dbd5-4336-a116-e4f6dd47c885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting weclouddata/\n",
      "/mnt/WCD has been unmounted.\n",
      "The bucket weclouddata/ was mounted to WCD \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, \"weclouddata/\", \"WCD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f032104-3911-4edd-ab6e-2170b2047777",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls /mnt/WCD/twitter/BlackFriday/2022/11/25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43cac92-0a26-4ed2-8ce2-2823afc7a8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, FloatType, StringType\n",
    "\n",
    "tweetSchema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('user_name', StringType(), True),\n",
    "    StructField('user_screen_name', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('followers_count', IntegerType(), True),\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('geo', StringType(), True),\n",
    "    StructField('created_at', StringType(), True) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1f9218b-79ce-4c20-93c5-6ad19703a2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = 'mnt/WCD/twitter/BlackFriday/2022/11/25/*/*'\n",
    "\n",
    "tweets = (spark.read\n",
    "        .option('header', 'false')\n",
    "        .option('delimiter', '\\t')\n",
    "        .schema(tweetSchema)\n",
    "        .csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e2f0b5-b527-4543-a781-f6bde2abc877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 848241"
     ]
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ff8e3c-292d-4b37-948e-587d09b94b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where geo column value is not 'None': 431\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Assuming 'geo' is the name of the column\n",
    "# non_null_geo_count = tweets.filter(col(\"geo\") != \"None\").count()\n",
    "\n",
    "# print(\"Number of rows where geo column value is not 'None':\", non_null_geo_count)\n",
    "# #431 is meaningless to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a154ab-710b-43c5-b0c2-9ea1b189403f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "tweets = tweets.select('text', 'followers_count','created_at')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49ae26d-e5ab-480a-8239-1db042f68386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in the 'created_at' column: 3054\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Count null values in the 'created_at' column\n",
    "# null_count = tweets.filter(col(\"created_at\").isNull()).count()\n",
    "\n",
    "# print(\"Number of null values in the 'created_at' column:\", null_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055740f6-06e8-41fe-8d3a-e9a6fa9047ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "\n",
    "# Define the UDF\n",
    "def is_retweet(text):\n",
    "    if re.match(r\"^RT\\b\", text):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Register the UDF\n",
    "is_retweet_udf = udf(is_retweet, IntegerType())\n",
    "\n",
    "# Apply the UDF to create a new column 'is_retweet'\n",
    "tweets = tweets.withColumn('is_retweet', is_retweet_udf(tweets['text']))\n",
    "tweets = tweets.na.drop(subset=['text'])\n",
    "tweets = tweets.na.drop(subset=['created_at'])\n",
    "# Show the DataFrame with the new column\n",
    "tweets.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4a13db-e509-4740-8ac8-4a6ba0f1e588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "\n",
    "# Assuming 'created_at' is the name of the column\n",
    "tweets_with_formatted_date = tweets.withColumn('created_at_ts', to_timestamp('created_at', 'EEE MMM dd HH:mm:ss Z yyyy')) \\\n",
    "                                    .withColumn('created_at_date', date_format('created_at_ts', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Display the DataFrame with the formatted date\n",
    "display(tweets_with_formatted_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f80d72-e616-4360-b07d-03e856631e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, trim\n",
    "\n",
    "# Define regex patterns for cleaning\n",
    "url_pattern = r\"http\\S+\"\n",
    "non_alpha_pattern = r\"[^a-zA-Z\\s]\"\n",
    "extra_space_pattern = r\"\\s+\"\n",
    "\n",
    "# Apply regex replacements\n",
    "tweets_clean = tweets.withColumn('text', regexp_replace('text', url_pattern, \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', non_alpha_pattern, \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', extra_space_pattern, \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text'))\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "display(tweets_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68aec068-ef2b-4a0a-81fa-69fdec667de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting TextBlob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.8->TextBlob) (8.0.4)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.4.28-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Requirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk>=3.8->TextBlob) (1.1.1)\n",
      "Installing collected packages: tqdm, regex, nltk, TextBlob\n",
      "Successfully installed TextBlob-0.18.0.post0 nltk-3.8.1 regex-2024.4.28 tqdm-4.66.4\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e504d02-c318-4f15-9ea3-77275297a2d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define a function to calculate sentiment scores using TextBlob\n",
    "def calculate_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_udf = udf(calculate_sentiment, FloatType())\n",
    "\n",
    "# Apply the UDF to create a new column 'sentiment'\n",
    "tweets_clean = tweets_clean.withColumn('sentiment', sentiment_udf(tweets_clean['text']))\n",
    "\n",
    "# Display the DataFrame with sentiment scores\n",
    "display(tweets_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e809ad-b6cc-4114-b017-ccd7c4447bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def map_sentiment(score):\n",
    "    if score >= 0.33:\n",
    "        return 1  # Positive sentiment\n",
    "    elif score <= -0.66:\n",
    "        return -1  # Negative sentiment\n",
    "    else:\n",
    "        return 0  # Neutral sentiment\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_integer_udf = udf(map_sentiment, IntegerType())\n",
    "\n",
    "# Convert sentiment scores to integers\n",
    "tweets_clean = tweets_clean.withColumn('sentiment_int', sentiment_integer_udf(tweets_clean['sentiment']))\n",
    "\n",
    "# Display the DataFrame with integer sentiment values\n",
    "display(tweets_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581e2957-9903-4345-bb69-7a126f6f18ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting weclouddata-test/\n",
      "/mnt/my_s3 has been unmounted.\n",
      "The bucket weclouddata-test/ was mounted to my_s3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, \"weclouddata-test/\", \"my_s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51917fa5-fb2a-41fd-a352-54f402af0046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cleanOut = \"mnt/my_s3/tweets_clean_1.csv\"\n",
    "\n",
    "(tweets_clean.write                       # Our DataFrameWriter\n",
    "  .option(\"delimiter\", \"\\t\")  \n",
    "  .option(\"header\", \"true\")\n",
    "  .mode(\"overwrite\")               # Replace existing files\n",
    "  .csv(cleanOut)               # Write DataFrame to csv files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a138638e-19b5-42a4-82b7-c373876d900f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cleanIn = \"/weclouddata-test/tweets_clean.csv\"\n",
    "cdr = (spark.read\n",
    "       .option(\"header\", \"true\")\n",
    "       .option(\"delimiter\", \"\\t\")\n",
    "       .csv(cleanIn)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7654ca15-c0a9-42e3-ac66-53b554a3e5e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d260f262-5d5f-4a0d-a71b-edd6a5367462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler, StopWordsRemover, HashingTF, IDF, Tokenizer, StringIndexer, NGram, ChiSqSelector, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Filter out rows with null values in the 'text' column\n",
    "tweets_clean = cdr.dropna(subset=[\"text\"])\n",
    "tweets_clean = tweets_clean.select('text', 'sentiment_int')\n",
    "# Define data splitting\n",
    "train, test = tweets_clean.randomSplit([0.9, 0.1], seed=20200819)\n",
    "# re sample\n",
    "# Define label encoding\n",
    "label_encoder = StringIndexer(inputCol=\"sentiment_int\", outputCol=\"label\")\n",
    "\n",
    "# Define pipeline stages\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5)\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"2gram\")\n",
    "ngram_hashingtf = HashingTF(inputCol=\"2gram\", outputCol=\"2gram_tf\", numFeatures=20000)\n",
    "ngram_idf = IDF(inputCol='2gram_tf', outputCol=\"2gram_idf\", minDocFreq=5) \n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\", \"2gram_tf\"], outputCol=\"rawFeatures\")\n",
    "selector = ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")\n",
    "\n",
    "# Define classifiers\n",
    "rf = RandomForestClassifier(numTrees=100)\n",
    "\n",
    "\n",
    "# Build pipeline with classifiers\n",
    "pipeline_rf = Pipeline(stages=[label_encoder, tokenizer, stopword_remover, cv, idf, ngram, ngram_hashingtf, ngram_idf, assembler, selector, rf])\n",
    "\n",
    "# Train the pipeline for RandomForestClassifier\n",
    "pipeline_model_rf = pipeline_rf.fit(train)\n",
    "\n",
    "# Make predictions for RandomForestClassifier\n",
    "prediction_rf = pipeline_model_rf.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ba57ec-eb9f-4cb3-bd45-e2ebf4b17744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy Score: 0.9290\n",
      "Random Forest - Precision: 0.9335\n",
      "Random Forest - Recall: 0.9290\n",
      "Random Forest - F1 Score: 0.9031\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate RandomForestClassifier\n",
    "evaluator_rf = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy_rf = evaluator_rf.evaluate(prediction_rf)\n",
    "\n",
    "# Precision\n",
    "precision_rf = evaluator_rf.evaluate(prediction_rf, {evaluator_rf.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Recall\n",
    "recall_rf = evaluator_rf.evaluate(prediction_rf, {evaluator_rf.metricName: \"weightedRecall\"})\n",
    "\n",
    "# F1-score\n",
    "f1_score_rf = evaluator_rf.evaluate(prediction_rf, {evaluator_rf.metricName: \"f1\"})\n",
    "\n",
    "# Print accuracy, precision, recall, and F1-score\n",
    "print(\"Random Forest - Accuracy Score: {0:.4f}\".format(accuracy_rf))\n",
    "print(\"Random Forest - Precision: {0:.4f}\".format(precision_rf))\n",
    "print(\"Random Forest - Recall: {0:.4f}\".format(recall_rf))\n",
    "print(\"Random Forest - F1 Score: {0:.4f}\".format(f1_score_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de590cb0-6a98-454d-9825-877dd4dbdc09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler, StopWordsRemover, HashingTF, IDF, Tokenizer, StringIndexer, NGram, ChiSqSelector, CountVectorizer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Filter out rows with null values in the 'text' column\n",
    "tweets_clean = cdr.dropna(subset=[\"text\"])\n",
    "tweets_clean = tweets_clean.select('text', 'sentiment_int')\n",
    "# Define data splitting\n",
    "train, test = tweets_clean.randomSplit([0.9, 0.1], seed=20200819)\n",
    "\n",
    "# Define label encoding\n",
    "label_encoder = StringIndexer(inputCol=\"sentiment_int\", outputCol=\"label\")\n",
    "\n",
    "# Define pipeline stages\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5)\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"2gram\")\n",
    "ngram_hashingtf = HashingTF(inputCol=\"2gram\", outputCol=\"2gram_tf\", numFeatures=20000)\n",
    "ngram_idf = IDF(inputCol='2gram_tf', outputCol=\"2gram_idf\", minDocFreq=5) \n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\", \"2gram_tf\"], outputCol=\"rawFeatures\")\n",
    "selector = ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")\n",
    "\n",
    "# Define classifier (Decision Tree)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Build pipeline with Decision Tree classifier\n",
    "pipeline_dt = Pipeline(stages=[label_encoder, tokenizer, stopword_remover, cv, idf, ngram, ngram_hashingtf, ngram_idf, assembler, selector, dt])\n",
    "\n",
    "# Train the pipeline for Decision Tree classifier\n",
    "pipeline_model_dt = pipeline_dt.fit(train)\n",
    "\n",
    "# Make predictions for Decision Tree classifier\n",
    "prediction_dt = pipeline_model_dt.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0abc036-d0ea-4a3a-ad22-2457a1f85da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy Score: 0.9485\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate DecisionTreeClassifier\n",
    "evaluator_dt = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy_dt = evaluator_dt.evaluate(prediction_dt)\n",
    "\n",
    "# Print accuracy, precision, recall, and F1-score for Decision Tree classifier\n",
    "print(\"Decision Tree - Accuracy Score: {0:.4f}\".format(accuracy_dt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdd2076-47ae-47bb-8dd6-a5cb53a78689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Filter out rows with null values in the 'text' column\n",
    "tweets_clean = cdr.dropna(subset=[\"text\"])\n",
    "tweets_clean = tweets_clean.select('text', 'sentiment_int')\n",
    "# Define data splitting\n",
    "train, test = tweets_clean.randomSplit([0.9, 0.1], seed=20200819)\n",
    "# Define classifier (Logistic Regression)\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "# Build pipeline with Logistic Regression classifier\n",
    "pipeline_lr = Pipeline(stages=[label_encoder, tokenizer, stopword_remover, cv, idf, ngram, ngram_hashingtf, ngram_idf, assembler, selector, lr])\n",
    "\n",
    "# Train the pipeline for Logistic Regression classifier\n",
    "pipeline_model_lr = pipeline_lr.fit(train)\n",
    "\n",
    "# Make predictions for Logistic Regression classifier\n",
    "prediction_lr = pipeline_model_lr.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb797c8-b9c4-4fdc-81fd-a5ea4d594664",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy Score: 0.9884\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression model\n",
    "accuracy_lr = evaluator_dt.evaluate(prediction_lr)\n",
    "\n",
    "# Print accuracy for Logistic Regression model\n",
    "print(\"Logistic Regression - Accuracy Score: {0:.4f}\".format(accuracy_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35abb2b-c551-4982-8122-59cc7117f311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#lr have highest score so pick it as base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c727ab5-86c2-41af-9fab-a1a784738c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy Score: 0.9884\n",
      "Logistic Regression - Precision: 0.9885\n",
      "Logistic Regression - Recall: 0.9884\n",
      "Logistic Regression - F1 Score: 0.9884\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate LogisticRegression\n",
    "evaluator_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator_lr.evaluate(prediction_lr)\n",
    "\n",
    "# Precision\n",
    "precision_lr = evaluator_lr.evaluate(prediction_lr, {evaluator_lr.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Recall\n",
    "recall_lr = evaluator_lr.evaluate(prediction_lr, {evaluator_lr.metricName: \"weightedRecall\"})\n",
    "\n",
    "# F1-score\n",
    "f1_score_lr = evaluator_lr.evaluate(prediction_lr, {evaluator_lr.metricName: \"f1\"})\n",
    "\n",
    "# Print accuracy, precision, recall, and F1-score\n",
    "print(\"Logistic Regression - Accuracy Score: {0:.4f}\".format(accuracy_lr))\n",
    "print(\"Logistic Regression - Precision: {0:.4f}\".format(precision_lr))\n",
    "print(\"Logistic Regression - Recall: {0:.4f}\".format(recall_lr))\n",
    "print(\"Logistic Regression - F1 Score: {0:.4f}\".format(f1_score_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83ea7ad-c9b1-49f9-9c99-6c0ac9250305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tweets_clean_lr = cdr.dropna(subset=[\"text\"])\n",
    "# transform for hole dataset\n",
    "predictions_lr_all = pipeline_model_lr.transform(tweets_clean_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5b5fdd-c8a5-4ac5-abae-ff1bc0253eaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "predictions_lr_to_csv = predictions_lr_all.select(\"text\", \"sentiment_int\",'label','prediction', 'followers_count','created_at','is_retweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03d1f36-0b91-44d5-8905-298e7e4917e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cleanOut = \"mnt/my_s3/prediction_lr/prediction_lr.csv\"\n",
    "\n",
    "(predictions_lr_to_csv.write                       # Our DataFrameWriter\n",
    "  .option(\"delimiter\", \"\\t\")  \n",
    "  .option(\"header\", \"true\")\n",
    "  .mode(\"overwrite\")               # Replace existing files\n",
    "  .csv(cleanOut)               # Write DataFrame to csv files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa243282-7916-437f-bb51-f8f5fc82d02e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#only one feature no need to check feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc4eadb-e4c0-4235-ae3f-328796937dda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns from the DataFrame\n",
    "predictions_all_to_csv = predictions_all.select(\"text\", \"sentiment_int\",'label','prediction', 'followers_count','created_at','is_retweet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebedd89d-43cc-48a2-bcfd-d6f1d2b080bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cleanOut = \"mnt/my_s3/prediction_all_1.csv\"\n",
    "\n",
    "(predictions_all_to_csv.write                       # Our DataFrameWriter\n",
    "  .option(\"delimiter\", \"\\t\")  \n",
    "  .option(\"header\", \"true\")\n",
    "  .mode(\"overwrite\")               # Replace existing files\n",
    "  .csv(cleanOut)               # Write DataFrame to csv files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c9ebc4-adc2-41c9-9c5c-c1b5796e3b46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = \"mnt/my_s3/prediction/*\"\n",
    "prediction_all_1 = (spark.read\n",
    "       .option(\"header\", \"true\")\n",
    "       .option(\"delimiter\", \"\\t\")\n",
    "       .csv(file)\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "weCloudDataHW",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
